\documentclass[a4paper]{scrartcl}
% \usepackage[left=1.5cm, right=1.5cm, top=1.5cm]{geometry}
\usepackage[bottom=2cm]{geometry}
\usepackage[l2tabu,orthodox]{nag}% Old habits die hard. All the same, there are commands, classes and packages which are outdated and superseded. nag provides routines to warn the user about the use of those.
\usepackage{listings, enumitem}
\usepackage{amsmath,tabu}
\usepackage[utf8]{inputenc}
\usepackage[all,error]{onlyamsmath}% Error on deprecated math commands like $$ $$.
\usepackage{fixltx2e}
\usepackage[strict=true]{csquotes}

\usepackage{color}
\usepackage[colorlinks=true]{hyperref}

\title{COMP9318 Assignment1}
\author{Fiona Lin z5131048}

\usepackage{graphicx}
\usepackage{subcaption}


\begin{document}
\maketitle

\paragraph{1. [40 marks]}
\label{sec:Question 1}
\paragraph{Solution}
\begin{enumerate}[label=\arabic*)]
  \item  (1) The tabular form with 4 attributes:\\
  \begin{tabular}{c |c | c | c | c}
    \hline
 & Location & Time & Item & SUM(Quantity)  \\
    \hline
    (location, time, item) & Sydney & 2005 & PS2 & 1400\\
    \hline
 & Sydney & 2006 & PS2 & 1500      \\
    \hline
 & Sydney & 2006 & Wii & 500 \\
    \hline
 & Melbourne & 2005 & Xbox 360 & 1700 \\
    \hline
 (location,item) & Sydney & All & PS2 & 2900 \\
    \hline
 & Sydney & All & Wii & 500\\
    \hline
 & Melbourne & All & Xbox 360 & 1700\\
    \hline
    (location,time) & Sydney & 2005 & All & 1400\\
    \hline
 & Sydney & 2006 & All & 2000\\
    \hline
 & Melbourne & 2005 & All & 1700\\
    \hline
    (time, item) & All & 2005 & PS2 & 1400\\
    \hline
 & All & 2006 & PS2 & 1500\\
    \hline
 & All & 2006 & Wii & 500\\
    \hline
 & All & 2005 & Xbox 360 & 1700 \\
    \hline
    (location) & Melbourne & All & All & 3400\\
    \hline
 & Sydney & All & All & 1700 \\
    \hline
    (time) & All & 2005 & All & 3100\\
    \hline
 & All & 2006 & All & 2000 \\
    \hline
    (item) & All & All & PS2 & 2900\\
    \hline
 & All & All & Wii & 500\\
    \hline
 & All & All & Xbox 360 & 1700\\
    \hline
    () & All & All & All & 5100\\
    \hline
  \end{tabular}\\
\\
\pagebreak
\\
 \item equivalent sql:\\
SELECT Location, Time, Item, SUM(Quantity)\\
FROM sales\\
GROUP BY\\
\hspace*{10mm} GROUPING SETS (\\
\hspace*{15mm} (Location, Time, Item),\\
\hspace*{15mm} (Location, Time),\\
\hspace*{15mm} (Location, Item),\\
\hspace*{15mm} (Time, Item),\\
\hspace*{15mm} (Location),\\
\hspace*{15mm} (Time),\\
\hspace*{15mm} (Item),\\
\hspace*{15mm} ()\\
\hspace*{10mm}  )\\
ORDER BY\\
Location, Time, Item;\\
  \item The result of the query in tabular form:\\
  \begin{tabular}{ c |c | c | c }
    \hline
    Location & Time & Item & SUM(Quantity) \\
    \hline
    Sydney & 2006 & All & 2000 \\
    \hline
    Sydney & All & All & 3400 \\
    \hline
    All & All & All & 5100 \\
    \hline
    Sydney & All & PS2 & 2900 \\
    \hline
    All & All & PS2 & 2900 \\
    \hline
    All & 2005 & All & 3100 \\
    \hline
    All & 2006 & All & 2000 \\
    \hline
\end{tabular}\\
\\
\item Since the function need to map a multi-dimensional point to a one-dimensioinal point, the function need to be bijective to allow the reverse mapping. \\
$f(Location,Time,Item) = Location*100+Time*10+Item$\\
\pagebreak
\\
\begin{tabular}{c  c  c  c c c | c| c}
  \hline Location & Time & Item & SUM(Quantity) & $f(Location,Time,Item)$  \\
  \hline 1 & 1 & 1 & 1400 & 111\\
  \hline 1 & 2 & 1 & 1500 & 121\\
  \hline 1 & 2 & 3 & 500 & 123\\
  \hline 2 & 1 & 2 & 1700 & 212\\
  \hline 1 & 0 & 1 & 2900 & 101\\
  \hline 1 & 0 & 3 & 500 & 103\\
  \hline 2 & 0 & 2 & 1700 & 202\\
  \hline 1 & 1 & 0 & 1400 & 110\\
  \hline 1 & 2 & 0 & 2000 & 120\\
  \hline 2 & 1 & 0 & 1700 & 210\\
  \hline 0 & 1 & 1 & 1400 & 11\\
  \hline 0 & 2 & 1 & 1500 & 21\\
  \hline 0 & 2 & 3 & 500 & 23\\
  \hline 0 & 1 & 2 & 1700 & 12\\
  \hline 2 & 0 & 0 & 3400 & 200\\
  \hline 1 & 0 & 0 & 1700 & 100\\
  \hline 0 & 1 & 0 & 3100 & 10\\
  \hline 0 & 2 & 0 & 2000 & 20\\
  \hline 0 & 0 & 1 & 2900 & 1\\
  \hline 0 & 0 & 3 & 500 & 3\\
  \hline 0 & 0 & 2 & 1700 & 2\\
  \hline 0 & 0 & 0 & 5100 & 0\\
  \hline
\end{tabular}\\
\\
The MOLAP cube (i.e., sparse multi-dimensional array) in a tabular form of  (ArrayIndex, Value).\\
\\
\begin{tabular}{ c | c}
  \hline index = $f(Location,Time,Item)$ & SUM(Quantity)   \\
  \hline 111 &1400 \\
  \hline 121 &1500 \\
  \hline 123 &1500 \\
  \hline 212 &1700 \\
  \hline 101 &2900 \\
  \hline 103 &1500 \\
  \hline 202 &1700 \\
  \hline 110 &1400\\
  \hline  120 &2000 \\
  \hline  210 &1700 \\
  \hline 11 &1400\\
  \hline 21 &1500 \\
  \hline 23 &500\\
  \hline 12 &1700 \\
  \hline 200 &3400 \\
  \hline 100 &1700 \\
  \hline 10 &3100 \\
  \hline 20 &2000 \\
  \hline 1 &2900 \\
  \hline 3 &500 \\
  \hline 2 &1700 \\
  \hline 0 &5100 \\
  \hline\end{tabular}\\
\end{enumerate}
\paragraph{2. [30 marks]}
\label{sec:Question 2}
\paragraph{Solution}
\begin{itemize}
  \item Proof: given $d$-dimensioin column vector
$\vec{x}= \begin{bmatrix} x_1 \\ x_2 \\ ... \\ x_d \end{bmatrix}$ as the input feature vector, and each dimensioin of $\textbf{x}$ takes only 2 values 0 or 1.\\
The Na\"ive Bayes classifier will product the label 0 iff $P(y=0|\textbf{x}) \geq P(y=1|\textbf{x})$, which equivalent:
\begin{align}
  \frac{P(\textbf{x}|y=0)P(y=0)}{P(\textbf{x}|y=1)P(y=1)} \geq 1
\end{align}
\text{Since } ${\displaystyle P(\textbf{x}|y)=\prod_{j=0}^{d} P(x_j|y)}$, then subsitute into (1), we have
\begin{align}
\frac{P(y=0)}{P(y=1)} \cdot {\displaystyle \prod_{j=0}^{d}} \frac{P(x_j|y=0)}{P(x_j|y=1)} \geq 1
\end{align}
In order to transform the inequality $(1)$ into the linear regression representation.\\
Let $P(y=0)=p$ and $P(x_j|y=0)=k_j$ also $P(x_j|y=1)=m_k$, because f the features are only 2 values 0 or 1, then
\begin{align*}
 & \ P(x_j|y=0)= k_j^{x_j}(1-k_j)^{(1-x_j)}\qquad \text{ and }\qquad P(x_j|y=1)= m_j^{x_j}(1-m_j)^{(1-x_j)}
\end{align*}
Hence (2) can rearrage as below:
\begin{align*}
 &\ \frac{p}{1-p} \cdot {\displaystyle \prod_{j=0}^{d}} \frac{k_j^{x_j}(1-k_j)^{(1-x_j)}}{m_j^{x_j}(1-m_j)^{(1-x_j)}} \geq 1\\
 &\ \frac{p}{1-p} \cdot {\displaystyle \prod_{j=0}^{d}} \frac{k_j^{x_j}(1-k_j)(1-k_j)^{(-x_j)}}{m_j^{x_j}(1-m_j)(1-m_j)^{(-x_j)}} \geq 1\\
 &\ \frac{p}{1-p} \cdot {\displaystyle \prod_{j=0}^{d}} \frac{k_j^{x_j}(1-k_j)(1-m_j)^{(x_j)}}{m_j^{x_j}(1-m_j)(1-k_j)^{(x_j)}} \geq 1\\
 &\ \Bigg(\frac{p}{1-p} {\displaystyle \prod_{j=0}^{d}} \frac{1-k_j}{1-m_j}\Bigg) \cdot {\displaystyle \prod_{j=0}^{d}} \Bigg(\frac{k_j(1-m_j)}{m_j(1-k_j)}\Bigg)^{x_j} \geq 1
\end{align*}
Taking logarithm on both sides, this become
\begin{align*}
 & \ \log{\Bigg(\frac{p}{1-p} {\displaystyle \prod_{j=0}^{d}} \frac{1-k_j}{1-m_j}\Bigg) \cdot {\displaystyle \prod_{j=0}^{d}} \Bigg(\frac{k_j(1-m_j)}{m_j(1-k_j)}\Bigg)^{x_j}} \geq 0\\
 & \ \log{\Bigg(\frac{p}{1-p} {\displaystyle \prod_{j=0}^{d}} \frac{1-k_j}{1-m_j}\Bigg)} + {\displaystyle \sum_{j=0}^{d}} \log{\Bigg(\frac{k_j(1-m_j)}{m_j(1-k_j)}\Bigg)^{x_j}} \geq 0\\
 & \ \log{\Bigg(\frac{p}{1-p} {\displaystyle \prod_{j=0}^{d}} \frac{1-k_j}{1-m_j}\Bigg)} + {\displaystyle \sum_{j=0}^{d}} x_j\log{\Bigg(\frac{k_j(1-m_j)}{m_j(1-k_j)}\Bigg)} \geq 0\\
 & \ \log{\Bigg(\frac{p}{1-p} {\displaystyle \prod_{j=0}^{d}} \frac{1-k_j}{1-m_j}\Bigg)} + x_0\log{\Bigg(\frac{k_0(1-m_0)}{m_0(1-k_0)}\Bigg)} + \\ &\ x_1\log{\Bigg(\frac{k_1(1-m_1)}{m_1(1-k_1)}\Bigg)}+...+ x_d\log{\Bigg(\frac{k_d(1-m_d)}{m_j(1-k_d)}\Bigg)}\geq 0
\end{align*}
As the first term does not have any $x_j$, so it is a constant for any input $\textbf{x}$.\\
Let's denote $b=\log{\Big(\frac{p}{1-p} {\displaystyle \prod_{j=0}^{d}} \frac{1-k_j}{1-m_j}\Big)}$ and $w_j=\log{\Big(\frac{k_j(1-m_j)}{m_j(1-k_j)}\Big)}$, the inequality $(1)$ transforms into the below linear regression representation\\
\begin{align*}
 & \ \log{\Bigg(\frac{p}{1-p} {\displaystyle \prod_{j=0}^{d}} \frac{1-k_j}{1-m_j}\Bigg)} + x_0\log{\Bigg(\frac{k_0(1-m_0)}{m_0(1-k_0)}\Bigg)} + \\ &\ x_1\log{\Bigg(\frac{k_1(1-m_1)}{m_1(1-k_1)}\Bigg)}+...+ x_d\log{\Bigg(\frac{k_d(1-m_d)}{m_j(1-k_d)}\Bigg)}\geq 0\\
 & \ \Rightarrow \\
 & \ b + x_0 w_0 + x_1 w_1 +...+ x_d w_d \geq 0\\
 & \ b + {\displaystyle \sum_{j=0}^{d}} x_j w_j \geq 0
\end{align*}
Therefore, if the feature vectors are $d$-dimension, then a Na\"ive Bayes classifier is
a linear classifier in a $d+1$-dimension space.
\item Proof: Given the parameters \textbf{$w_{LR}$} and \textbf{$w_{NB}$}, we know from previous
\begin{align*}
w_{NB}=\log{\Big(\frac{k_j(1-m_j)}{m_j(1-k_j)}\Big)}
\end{align*}
While we also know that \textbf{$w_{LR}$} is obtained by finding the minimum of the \textsl{sqared error function} $J$ with the learning rate $\lambda$, so its formular of updating rule
\begin{align*}
  w_{LR}= w_{LR} - \lambda \frac{\partial J(w) }{\partial w_j}
\end{align*}
Obviously, it can easily compute \textbf{$w_{NB}$} in only 1 operation, while \textbf{$w_{LR}$} needs to compute all the possible \textbf{$w_{LR}$} and find the minimum one.\\
\\*
Hence, learning \textbf{$w_{NB}$} is much easier than learning \textbf{$w_{LR}$}.
\end{itemize}
\paragraph{3. [30 marks]}
\label{sec:Question 3}
\paragraph{Solution}
\begin{enumerate}[label=(\arabic*)]
  \item
  Let $y_m$ denote as our model for the sample of a mixture of $S_1$ and $S_2$ , and $q_1,\ q_2$ be the parameters $\theta$. After measuring those 3 components we have obtained the percentages as ${\{u_j\}_{j=1}^{m}}$, and let  $y_o$ as the samples our observed;
  Then we have,
  \begin{align*}
    &\ y_m = \sum_{j=1}^{m} (q_1* p_{1,j} + q_2* p_{2,j})= \sum_{j=1}^{m} (q_1* p_{1,j} + (1-q_1)* p_{2,j})\\ &\
     y_o = \sum_{j=1}^{m} u_j\\ &\
  \end{align*}
  Then let $p(\theta)=Pr(y_m|\theta)$ be the probability of some $y_o$ being measured from sample with the model $y_m$ and the given $\theta$, assuming all $n$ measured instances are $y_1,y_2,...,y_n$ independently. \\
  Any $\theta$ is possible, but the best $\theta$ is more likely. In order to find the best $\theta$, we need to maximise the likelihood $p(\theta)$ which means to be observed more measurement $y_o$ close to $y_m$ for some $\theta$. From sample measurements, we know the probability $p_o$ of $y_o$ being measured/observed\\
  Hence,
  \begin{align*}
   &\ L(\theta)=\prod_{o=1}^{n}P(y_1,y_2,...,y_n|\theta)=\binom{n}{p_on}\cdot p(\theta)^{n*p_o}\cdot (1-p(\theta))^{(1-p_o)n}\\ 
  \end{align*}
%   Without any other knowledge or information, we assume $P(y_o|\theta)$ follows a normal distribution $\mathcal{N}(\mu,\,\sigma^{2})$
  \\
  The log-likelihood function:\\ 
  \begin{align*}
   &\ l(y_o|\theta)=\sum_{o=1}^{n}\log{P(y_o|\theta)}=\sum_{o=1}^{n}\log{\bigg( \frac{1}{\sigma \sqrt {2\pi}} \textbf{e}^{- \frac{(y_o-y_m)^2}{2\sigma^2}}\bigg)}\\ 
   &\ \qquad=\sum_{o=1}^{n}\log{\bigg( \bigg)}\\ &\
   % \log{\frac{p_o}{1-p_o}}=\log{\frac{p_o}{p_m}}=\log{\Bigg( \binom{n}{n p_o} \cdot (p_m)^{np_o} \cdot (1-p_m)^{(1-p_o)n} \Bigg)}\\
   %  &\ \log{P(y_o|\theta)} = \log{\frac{p_o}{1-p_o}} \Rightarrow P(y_0|\theta)= \frac{e^{\log{y_m}}}{1-e^{\log{y_m}} nb}\\ &\
  \end{align*}
%   \[ \frac{\partial u}{\partial t}
%   = h^2 \left( \frac{\partial^2 u}{\partial x^2}
%      + \frac{\partial^2 u}{\partial y^2}
%      + \frac{\partial^2 u}{\partial z^2} \right) \]
  \item
\end{enumerate}
\end{document}
